{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e943846c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.004224,
     "end_time": "2025-09-28T12:44:18.245241",
     "exception": false,
     "start_time": "2025-09-28T12:44:18.241017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddba0ca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T12:44:18.254289Z",
     "iopub.status.busy": "2025-09-28T12:44:18.253950Z",
     "iopub.status.idle": "2025-09-28T12:44:18.275233Z",
     "shell.execute_reply": "2025-09-28T12:44:18.273884Z"
    },
    "papermill": {
     "duration": 0.02792,
     "end_time": "2025-09-28T12:44:18.277329",
     "exception": false,
     "start_time": "2025-09-28T12:44:18.249409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets processed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Paths to your datasets\n",
    "data_folders = [\n",
    "    \"/kaggle/input/test-labeled\",\n",
    "    \"/kaggle/input/test-unlabeled\",\n",
    "    \"/kaggle/input/train-data\"\n",
    "]\n",
    "\n",
    "output_path = \"/kaggle/working/\"  # Extracting files here\n",
    "\n",
    "# Unzip all files\n",
    "for folder in data_folders:\n",
    "    if os.path.exists(folder):  # Check if the folder exists\n",
    "        for file in os.listdir(folder):\n",
    "            if file.endswith(\".zip\"):\n",
    "                file_path = os.path.join(folder, file)\n",
    "                extract_path = os.path.join(output_path, os.path.splitext(file)[0])  # Extract to a subfolder\n",
    "\n",
    "                if not os.path.exists(extract_path):  # Avoid re-extracting\n",
    "                    try:\n",
    "                        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                            zip_ref.extractall(extract_path)\n",
    "                        print(f\"Extracted: {file} to {extract_path}\")\n",
    "                    except zipfile.BadZipFile:\n",
    "                        print(f\"Error: Corrupted zip file - {file}\")\n",
    "                else:\n",
    "                    print(f\"Skipping extraction (already exists): {file}\")\n",
    "\n",
    "print(\"All datasets processed successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44595e8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T12:44:18.285906Z",
     "iopub.status.busy": "2025-09-28T12:44:18.285566Z",
     "iopub.status.idle": "2025-09-28T12:44:20.384050Z",
     "shell.execute_reply": "2025-09-28T12:44:20.382928Z"
    },
    "papermill": {
     "duration": 2.105134,
     "end_time": "2025-09-28T12:44:20.385983",
     "exception": false,
     "start_time": "2025-09-28T12:44:18.280849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MU-SHROOM DATA LOADING SUMMARY\n",
      "==================================================\n",
      "Datasets loaded: 3\n",
      "Total files: 32\n",
      "Total records: 6305\n",
      "\n",
      "DATASET BREAKDOWN:\n",
      "  test_labeled:\n",
      "    Files: 14\n",
      "    Records: 1902\n",
      "    Common keys: ['model_output_tokens', 'hard_labels', 'soft_labels', 'lang', 'model_input', 'model_output_logits', 'model_output_text', 'model_id', 'id']\n",
      "  test_unlabeled:\n",
      "    Files: 14\n",
      "    Records: 1902\n",
      "    Common keys: ['model_output_tokens', 'lang', 'model_input', 'model_output_logits', 'model_output_text', 'model_id', 'id']\n",
      "  train_data:\n",
      "    Files: 4\n",
      "    Records: 2501\n",
      "    Common keys: ['model_output_tokens', 'lang', 'model_input', 'model_output_logits', 'model_output_text', 'model_id']\n",
      "\n",
      "Data loading complete. Ready for preprocessing and feature engineering.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MuShroomDataLoader:\n",
    "    \"\"\"\n",
    "    Professional data loader for Mu-SHROOM hallucination detection project.\n",
    "    Handles multiple file formats and provides comprehensive dataset analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_paths=None):\n",
    "        \"\"\"\n",
    "        Initialize the data loader with specified base paths.\n",
    "        \n",
    "        Args:\n",
    "            base_paths (dict): Dictionary mapping dataset names to their paths\n",
    "        \"\"\"\n",
    "        if base_paths is None:\n",
    "            self.base_paths = {\n",
    "                'test_labeled': '/kaggle/input/test-labeled/v1/',\n",
    "                'test_unlabeled': '/kaggle/input/test-unlabeled/v1/',\n",
    "                'train_data': '/kaggle/input/train-data/train/'\n",
    "            }\n",
    "        else:\n",
    "            self.base_paths = base_paths\n",
    "            \n",
    "        self.loaded_datasets = {}\n",
    "        self.file_registry = {}\n",
    "        \n",
    "    def discover_files(self):\n",
    "        \"\"\"\n",
    "        Discover all files in the specified data directories.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary mapping dataset names to lists of file information\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting file discovery process\")\n",
    "        \n",
    "        discovered_files = {}\n",
    "        \n",
    "        for dataset_name, path in self.base_paths.items():\n",
    "            logger.info(f\"Exploring dataset: {dataset_name}\")\n",
    "            logger.info(f\"Path: {path}\")\n",
    "            \n",
    "            if not os.path.exists(path):\n",
    "                logger.warning(f\"Path does not exist: {path}\")\n",
    "                discovered_files[dataset_name] = []\n",
    "                continue\n",
    "            \n",
    "            files = []\n",
    "            \n",
    "            try:\n",
    "                for root, dirs, filenames in os.walk(path):\n",
    "                    for filename in filenames:\n",
    "                        file_path = os.path.join(root, filename)\n",
    "                        file_size = os.path.getsize(file_path)\n",
    "                        file_ext = os.path.splitext(filename)[1].lower()\n",
    "                        \n",
    "                        relative_path = os.path.relpath(file_path, path)\n",
    "                        \n",
    "                        file_info = {\n",
    "                            'name': relative_path,\n",
    "                            'full_path': file_path,\n",
    "                            'size_bytes': file_size,\n",
    "                            'extension': file_ext,\n",
    "                            'dataset': dataset_name\n",
    "                        }\n",
    "                        \n",
    "                        files.append(file_info)\n",
    "                        logger.debug(f\"Found file: {relative_path} ({file_size} bytes)\")\n",
    "                \n",
    "                logger.info(f\"Dataset {dataset_name}: Found {len(files)} files\")\n",
    "                discovered_files[dataset_name] = files\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error exploring {dataset_name}: {str(e)}\")\n",
    "                discovered_files[dataset_name] = []\n",
    "        \n",
    "        self.file_registry = discovered_files\n",
    "        return discovered_files\n",
    "    \n",
    "    def load_json_file(self, file_path, max_records=None):\n",
    "        \"\"\"\n",
    "        Load a JSON file and return its contents.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the JSON file\n",
    "            max_records (int, optional): Maximum number of records to load\n",
    "            \n",
    "        Returns:\n",
    "            dict or list: Loaded JSON data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            if isinstance(data, list) and max_records is not None:\n",
    "                data = data[:max_records]\n",
    "            \n",
    "            logger.info(f\"Successfully loaded JSON file: {os.path.basename(file_path)}\")\n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading JSON file {file_path}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def load_jsonl_file(self, file_path, max_records=None):\n",
    "        \"\"\"\n",
    "        Load a JSONL file and return its contents as a list.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the JSONL file\n",
    "            max_records (int, optional): Maximum number of records to load\n",
    "            \n",
    "        Returns:\n",
    "            list: List of JSON objects\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data = []\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    if max_records is not None and i >= max_records:\n",
    "                        break\n",
    "                    \n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        data.append(json.loads(line))\n",
    "            \n",
    "            logger.info(f\"Successfully loaded JSONL file: {os.path.basename(file_path)} ({len(data)} records)\")\n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading JSONL file {file_path}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def load_csv_file(self, file_path, max_records=None):\n",
    "        \"\"\"\n",
    "        Load a CSV file and return as pandas DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the CSV file\n",
    "            max_records (int, optional): Maximum number of records to load\n",
    "            \n",
    "        Returns:\n",
    "            pandas.DataFrame: Loaded CSV data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if max_records is not None:\n",
    "                data = pd.read_csv(file_path, nrows=max_records)\n",
    "            else:\n",
    "                data = pd.read_csv(file_path)\n",
    "            \n",
    "            logger.info(f\"Successfully loaded CSV file: {os.path.basename(file_path)} ({data.shape[0]} rows)\")\n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading CSV file {file_path}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_and_load_zip(self, file_path, max_records=None):\n",
    "        \"\"\"\n",
    "        Extract and load contents from a ZIP file.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the ZIP file\n",
    "            max_records (int, optional): Maximum number of records to load per file\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary mapping filenames to their loaded data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            zip_contents = {}\n",
    "            \n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                file_list = zip_ref.namelist()\n",
    "                logger.info(f\"ZIP file contains {len(file_list)} files\")\n",
    "                \n",
    "                for zip_filename in file_list:\n",
    "                    if zip_filename.endswith('/'):  # Skip directories\n",
    "                        continue\n",
    "                    \n",
    "                    file_ext = os.path.splitext(zip_filename)[1].lower()\n",
    "                    \n",
    "                    try:\n",
    "                        if file_ext == '.json':\n",
    "                            with zip_ref.open(zip_filename) as f:\n",
    "                                data = json.load(f)\n",
    "                                if isinstance(data, list) and max_records is not None:\n",
    "                                    data = data[:max_records]\n",
    "                                zip_contents[zip_filename] = data\n",
    "                                \n",
    "                        elif file_ext == '.jsonl':\n",
    "                            data = []\n",
    "                            with zip_ref.open(zip_filename) as f:\n",
    "                                for i, line in enumerate(f):\n",
    "                                    if max_records is not None and i >= max_records:\n",
    "                                        break\n",
    "                                    line = line.decode('utf-8').strip()\n",
    "                                    if line:\n",
    "                                        data.append(json.loads(line))\n",
    "                            zip_contents[zip_filename] = data\n",
    "                            \n",
    "                        logger.debug(f\"Loaded from ZIP: {zip_filename}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Could not load {zip_filename} from ZIP: {str(e)}\")\n",
    "            \n",
    "            logger.info(f\"Successfully processed ZIP file: {os.path.basename(file_path)}\")\n",
    "            return zip_contents\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing ZIP file {file_path}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def load_file(self, file_info, max_records=1000):\n",
    "        \"\"\"\n",
    "        Load a single file based on its extension.\n",
    "        \n",
    "        Args:\n",
    "            file_info (dict): File information dictionary\n",
    "            max_records (int): Maximum number of records to load\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (success, data) where success is boolean and data is the loaded content\n",
    "        \"\"\"\n",
    "        file_path = file_info['full_path']\n",
    "        extension = file_info['extension']\n",
    "        \n",
    "        # Skip very small files (likely metadata or empty)\n",
    "        if file_info['size_bytes'] < 50:\n",
    "            logger.debug(f\"Skipping small file: {file_info['name']} ({file_info['size_bytes']} bytes)\")\n",
    "            return False, None\n",
    "        \n",
    "        if extension == '.json':\n",
    "            data = self.load_json_file(file_path, max_records)\n",
    "            return data is not None, data\n",
    "            \n",
    "        elif extension == '.jsonl':\n",
    "            data = self.load_jsonl_file(file_path, max_records)\n",
    "            return data is not None, data\n",
    "            \n",
    "        elif extension == '.csv':\n",
    "            data = self.load_csv_file(file_path, max_records)\n",
    "            return data is not None, data\n",
    "            \n",
    "        elif extension == '.zip':\n",
    "            data = self.extract_and_load_zip(file_path, max_records)\n",
    "            return data is not None, data\n",
    "            \n",
    "        else:\n",
    "            logger.debug(f\"Unsupported file type: {extension}\")\n",
    "            return False, None\n",
    "    \n",
    "    def load_all_datasets(self, max_records_per_file=1000):\n",
    "        \"\"\"\n",
    "        Load all discovered datasets.\n",
    "        \n",
    "        Args:\n",
    "            max_records_per_file (int): Maximum records to load per file\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary mapping dataset names to their loaded data\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting dataset loading process\")\n",
    "        \n",
    "        if not self.file_registry:\n",
    "            self.discover_files()\n",
    "        \n",
    "        loaded_data = {}\n",
    "        \n",
    "        for dataset_name, files in self.file_registry.items():\n",
    "            logger.info(f\"Loading dataset: {dataset_name}\")\n",
    "            \n",
    "            dataset_data = {}\n",
    "            successful_loads = 0\n",
    "            \n",
    "            for file_info in files:\n",
    "                success, data = self.load_file(file_info, max_records_per_file)\n",
    "                \n",
    "                if success:\n",
    "                    dataset_data[file_info['name']] = {\n",
    "                        'data': data,\n",
    "                        'file_info': file_info,\n",
    "                        'loaded_at': pd.Timestamp.now()\n",
    "                    }\n",
    "                    successful_loads += 1\n",
    "            \n",
    "            loaded_data[dataset_name] = dataset_data\n",
    "            logger.info(f\"Dataset {dataset_name}: Successfully loaded {successful_loads}/{len(files)} files\")\n",
    "        \n",
    "        self.loaded_datasets = loaded_data\n",
    "        return loaded_data\n",
    "    \n",
    "    def analyze_dataset_structure(self, dataset_name=None):\n",
    "        \"\"\"\n",
    "        Analyze the structure of loaded datasets.\n",
    "        \n",
    "        Args:\n",
    "            dataset_name (str, optional): Specific dataset to analyze. If None, analyzes all.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Analysis results\n",
    "        \"\"\"\n",
    "        if not self.loaded_datasets:\n",
    "            logger.warning(\"No datasets loaded. Call load_all_datasets() first.\")\n",
    "            return {}\n",
    "        \n",
    "        datasets_to_analyze = [dataset_name] if dataset_name else list(self.loaded_datasets.keys())\n",
    "        analysis_results = {}\n",
    "        \n",
    "        for ds_name in datasets_to_analyze:\n",
    "            if ds_name not in self.loaded_datasets:\n",
    "                logger.warning(f\"Dataset {ds_name} not found in loaded datasets\")\n",
    "                continue\n",
    "            \n",
    "            logger.info(f\"Analyzing dataset structure: {ds_name}\")\n",
    "            \n",
    "            dataset = self.loaded_datasets[ds_name]\n",
    "            analysis = {\n",
    "                'file_count': len(dataset),\n",
    "                'total_records': 0,\n",
    "                'record_types': {},\n",
    "                'common_keys': None,\n",
    "                'sample_records': [],\n",
    "                'data_schema': {}\n",
    "            }\n",
    "            \n",
    "            all_keys_sets = []\n",
    "            \n",
    "            for filename, file_data in dataset.items():\n",
    "                data = file_data['data']\n",
    "                \n",
    "                if isinstance(data, list):\n",
    "                    analysis['total_records'] += len(data)\n",
    "                    analysis['record_types'][filename] = f\"list ({len(data)} items)\"\n",
    "                    \n",
    "                    if len(data) > 0 and isinstance(data[0], dict):\n",
    "                        keys = set(data[0].keys())\n",
    "                        all_keys_sets.append(keys)\n",
    "                        \n",
    "                        # Store sample records\n",
    "                        analysis['sample_records'].extend(data[:2])\n",
    "                        \n",
    "                        # Analyze data types\n",
    "                        for key, value in data[0].items():\n",
    "                            if key not in analysis['data_schema']:\n",
    "                                analysis['data_schema'][key] = set()\n",
    "                            analysis['data_schema'][key].add(type(value).__name__)\n",
    "                \n",
    "                elif isinstance(data, dict):\n",
    "                    # Handle ZIP contents or nested dictionaries\n",
    "                    for sub_key, sub_data in data.items():\n",
    "                        if isinstance(sub_data, list) and len(sub_data) > 0:\n",
    "                            analysis['total_records'] += len(sub_data)\n",
    "                            if isinstance(sub_data[0], dict):\n",
    "                                keys = set(sub_data[0].keys())\n",
    "                                all_keys_sets.append(keys)\n",
    "                                analysis['sample_records'].extend(sub_data[:2])\n",
    "                \n",
    "                elif isinstance(data, pd.DataFrame):\n",
    "                    analysis['total_records'] += len(data)\n",
    "                    analysis['record_types'][filename] = f\"dataframe ({data.shape[0]}x{data.shape[1]})\"\n",
    "                    keys = set(data.columns)\n",
    "                    all_keys_sets.append(keys)\n",
    "                    analysis['sample_records'].extend(data.head(2).to_dict('records'))\n",
    "            \n",
    "            # Find common keys across all files\n",
    "            if all_keys_sets:\n",
    "                analysis['common_keys'] = set.intersection(*all_keys_sets) if all_keys_sets else set()\n",
    "                analysis['all_unique_keys'] = set.union(*all_keys_sets) if all_keys_sets else set()\n",
    "            \n",
    "            # Clean up data schema\n",
    "            for key, types in analysis['data_schema'].items():\n",
    "                analysis['data_schema'][key] = list(types)\n",
    "            \n",
    "            analysis_results[ds_name] = analysis\n",
    "            \n",
    "            # Log analysis summary\n",
    "            logger.info(f\"Dataset {ds_name} analysis:\")\n",
    "            logger.info(f\"  - Files: {analysis['file_count']}\")\n",
    "            logger.info(f\"  - Total records: {analysis['total_records']}\")\n",
    "            logger.info(f\"  - Common keys: {len(analysis['common_keys'])} keys\")\n",
    "            logger.info(f\"  - Unique keys across all files: {len(analysis.get('all_unique_keys', set()))} keys\")\n",
    "        \n",
    "        return analysis_results\n",
    "    \n",
    "    def get_summary_report(self):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive summary report of all loaded data.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Summary report\n",
    "        \"\"\"\n",
    "        if not self.loaded_datasets:\n",
    "            return {\"error\": \"No datasets loaded\"}\n",
    "        \n",
    "        total_files = 0\n",
    "        total_records = 0\n",
    "        datasets_summary = {}\n",
    "        \n",
    "        analysis = self.analyze_dataset_structure()\n",
    "        \n",
    "        for dataset_name, dataset_analysis in analysis.items():\n",
    "            total_files += dataset_analysis['file_count']\n",
    "            total_records += dataset_analysis['total_records']\n",
    "            \n",
    "            datasets_summary[dataset_name] = {\n",
    "                'files': dataset_analysis['file_count'],\n",
    "                'records': dataset_analysis['total_records'],\n",
    "                'common_keys': list(dataset_analysis['common_keys']),\n",
    "                'sample_schema': dataset_analysis['data_schema']\n",
    "            }\n",
    "        \n",
    "        summary = {\n",
    "            'total_datasets': len(self.loaded_datasets),\n",
    "            'total_files': total_files,\n",
    "            'total_records': total_records,\n",
    "            'datasets': datasets_summary,\n",
    "            'analysis_timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function for the data loader.\n",
    "    \"\"\"\n",
    "    # Initialize the data loader\n",
    "    loader = MuShroomDataLoader()\n",
    "    \n",
    "    # Discover all files\n",
    "    logger.info(\"MU-SHROOM Data Loading Pipeline - Starting\")\n",
    "    discovered_files = loader.discover_files()\n",
    "    \n",
    "    # Load all datasets\n",
    "    loaded_data = loader.load_all_datasets(max_records_per_file=1000)\n",
    "    \n",
    "    # Generate analysis\n",
    "    analysis_results = loader.analyze_dataset_structure()\n",
    "    \n",
    "    # Generate summary report\n",
    "    summary = loader.get_summary_report()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nMU-SHROOM DATA LOADING SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Datasets loaded: {summary['total_datasets']}\")\n",
    "    print(f\"Total files: {summary['total_files']}\")\n",
    "    print(f\"Total records: {summary['total_records']}\")\n",
    "    \n",
    "    print(\"\\nDATASET BREAKDOWN:\")\n",
    "    for dataset_name, info in summary['datasets'].items():\n",
    "        print(f\"  {dataset_name}:\")\n",
    "        print(f\"    Files: {info['files']}\")\n",
    "        print(f\"    Records: {info['records']}\")\n",
    "        print(f\"    Common keys: {info['common_keys']}\")\n",
    "    \n",
    "    return loader, loaded_data, analysis_results, summary\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    loader, data, analysis, summary = main()\n",
    "    print(\"\\nData loading complete. Ready for preprocessing and feature engineering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5667b5",
   "metadata": {
    "papermill": {
     "duration": 0.002962,
     "end_time": "2025-09-28T12:44:20.392429",
     "exception": false,
     "start_time": "2025-09-28T12:44:20.389467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52d3e0e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T12:44:20.400885Z",
     "iopub.status.busy": "2025-09-28T12:44:20.400297Z",
     "iopub.status.idle": "2025-09-28T12:44:23.143780Z",
     "shell.execute_reply": "2025-09-28T12:44:23.142673Z"
    },
    "papermill": {
     "duration": 2.750168,
     "end_time": "2025-09-28T12:44:23.145805",
     "exception": false,
     "start_time": "2025-09-28T12:44:20.395637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Mu-SHROOM pipeline ready! Use: results = run_simple_pipeline(loaded_data)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SimpleMuShroomProcessor:\n",
    "    \"\"\"\n",
    "    Simplified Mu-SHROOM processor that works without external NLTK dependencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = self._get_stopwords()\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def _get_stopwords(self):\n",
    "        \"\"\"Basic English stopwords.\"\"\"\n",
    "        return {\n",
    "            'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "            'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n",
    "            'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom',\n",
    "            'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "            'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "            'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at',\n",
    "            'by', 'for', 'with', 'through', 'during', 'before', 'after', 'above', 'below',\n",
    "            'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "            'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor',\n",
    "            'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just'\n",
    "        }\n",
    "    \n",
    "    def simple_tokenize(self, text):\n",
    "        \"\"\"Simple tokenization without NLTK.\"\"\"\n",
    "        if pd.isna(text) or not text:\n",
    "            return []\n",
    "        text = str(text).lower()\n",
    "        # Remove punctuation and split\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        return [word for word in text.split() if word]\n",
    "    \n",
    "    def simple_sentence_split(self, text):\n",
    "        \"\"\"Simple sentence splitting.\"\"\"\n",
    "        if pd.isna(text) or not text:\n",
    "            return []\n",
    "        sentences = re.split(r'[.!?]+', str(text))\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    def extract_basic_features(self, text):\n",
    "        \"\"\"Extract basic text features.\"\"\"\n",
    "        if pd.isna(text) or not text:\n",
    "            return {\n",
    "                'char_count': 0, 'word_count': 0, 'sentence_count': 0,\n",
    "                'avg_word_length': 0, 'avg_sentence_length': 0,\n",
    "                'punctuation_count': 0, 'uppercase_ratio': 0, 'digit_count': 0,\n",
    "                'stopword_ratio': 0, 'exclamation_count': 0, 'question_count': 0\n",
    "            }\n",
    "        \n",
    "        text = str(text)\n",
    "        words = self.simple_tokenize(text)\n",
    "        sentences = self.simple_sentence_split(text)\n",
    "        \n",
    "        # Basic statistics\n",
    "        char_count = len(text)\n",
    "        word_count = len(words)\n",
    "        sentence_count = max(len(sentences), 1)  # Avoid division by zero\n",
    "        \n",
    "        avg_word_length = np.mean([len(word) for word in words]) if words else 0\n",
    "        avg_sentence_length = word_count / sentence_count\n",
    "        \n",
    "        # Count features\n",
    "        punctuation_count = sum(1 for char in text if char in '.,!?;:')\n",
    "        uppercase_count = sum(1 for char in text if char.isupper())\n",
    "        uppercase_ratio = uppercase_count / char_count if char_count > 0 else 0\n",
    "        digit_count = sum(1 for char in text if char.isdigit())\n",
    "        \n",
    "        # Stopword ratio\n",
    "        stopword_count = sum(1 for word in words if word in self.stop_words)\n",
    "        stopword_ratio = stopword_count / word_count if word_count > 0 else 0\n",
    "        \n",
    "        # Special punctuation\n",
    "        exclamation_count = text.count('!')\n",
    "        question_count = text.count('?')\n",
    "        \n",
    "        return {\n",
    "            'char_count': char_count,\n",
    "            'word_count': word_count,\n",
    "            'sentence_count': sentence_count,\n",
    "            'avg_word_length': float(avg_word_length),\n",
    "            'avg_sentence_length': float(avg_sentence_length),\n",
    "            'punctuation_count': punctuation_count,\n",
    "            'uppercase_ratio': float(uppercase_ratio),\n",
    "            'digit_count': digit_count,\n",
    "            'stopword_ratio': float(stopword_ratio),\n",
    "            'exclamation_count': exclamation_count,\n",
    "            'question_count': question_count\n",
    "        }\n",
    "    \n",
    "    def extract_similarity_features(self, input_text, output_text):\n",
    "        \"\"\"Extract similarity features between input and output.\"\"\"\n",
    "        if pd.isna(input_text) or pd.isna(output_text) or not input_text or not output_text:\n",
    "            return {\n",
    "                'tfidf_similarity': 0.0, 'word_overlap_jaccard': 0.0,\n",
    "                'word_overlap_dice': 0.0, 'length_ratio': 0.0, 'compression_ratio': 1.0\n",
    "            }\n",
    "        \n",
    "        input_text = str(input_text)\n",
    "        output_text = str(output_text)\n",
    "        \n",
    "        # TF-IDF similarity\n",
    "        try:\n",
    "            tfidf_matrix = self.tfidf_vectorizer.fit_transform([input_text, output_text])\n",
    "            tfidf_similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        except:\n",
    "            tfidf_similarity = 0.0\n",
    "        \n",
    "        # Word overlap\n",
    "        input_words = set(self.simple_tokenize(input_text))\n",
    "        output_words = set(self.simple_tokenize(output_text))\n",
    "        \n",
    "        intersection = len(input_words & output_words)\n",
    "        union = len(input_words | output_words)\n",
    "        \n",
    "        jaccard_similarity = intersection / union if union > 0 else 0.0\n",
    "        dice_similarity = (2 * intersection) / (len(input_words) + len(output_words)) if (len(input_words) + len(output_words)) > 0 else 0.0\n",
    "        \n",
    "        # Length ratios\n",
    "        length_ratio = len(output_text) / len(input_text) if len(input_text) > 0 else 0.0\n",
    "        compression_ratio = len(input_text) / len(output_text) if len(output_text) > 0 else 1.0\n",
    "        \n",
    "        return {\n",
    "            'tfidf_similarity': float(tfidf_similarity),\n",
    "            'word_overlap_jaccard': float(jaccard_similarity),\n",
    "            'word_overlap_dice': float(dice_similarity),\n",
    "            'length_ratio': float(length_ratio),\n",
    "            'compression_ratio': float(compression_ratio)\n",
    "        }\n",
    "    \n",
    "    def extract_logits_features(self, logits):\n",
    "        \"\"\"Extract features from logits.\"\"\"\n",
    "        if pd.isna(logits) or not logits:\n",
    "            return {\n",
    "                'logits_mean': 0.0, 'logits_std': 0.0, 'logits_max': 0.0,\n",
    "                'logits_min': 0.0, 'logits_range': 0.0, 'logits_entropy': 0.0\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            if isinstance(logits, str):\n",
    "                logits = json.loads(logits)\n",
    "            \n",
    "            logits_array = np.array(logits, dtype=float)\n",
    "            \n",
    "            # Basic statistics\n",
    "            logits_mean = np.mean(logits_array)\n",
    "            logits_std = np.std(logits_array)\n",
    "            logits_max = np.max(logits_array)\n",
    "            logits_min = np.min(logits_array)\n",
    "            logits_range = logits_max - logits_min\n",
    "            \n",
    "            # Entropy (uncertainty measure)\n",
    "            probs = np.exp(logits_array) / np.sum(np.exp(logits_array))\n",
    "            entropy = -np.sum(probs * np.log(probs + 1e-10))\n",
    "            \n",
    "            return {\n",
    "                'logits_mean': float(logits_mean),\n",
    "                'logits_std': float(logits_std),\n",
    "                'logits_max': float(logits_max),\n",
    "                'logits_min': float(logits_min),\n",
    "                'logits_range': float(logits_range),\n",
    "                'logits_entropy': float(entropy)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error processing logits: {e}\")\n",
    "            return {\n",
    "                'logits_mean': 0.0, 'logits_std': 0.0, 'logits_max': 0.0,\n",
    "                'logits_min': 0.0, 'logits_range': 0.0, 'logits_entropy': 0.0\n",
    "            }\n",
    "    \n",
    "    def extract_hallucination_indicators(self, input_text, output_text):\n",
    "        \"\"\"Extract hallucination indicator features.\"\"\"\n",
    "        if pd.isna(input_text) or pd.isna(output_text) or not input_text or not output_text:\n",
    "            return {'new_numbers': 0, 'new_caps_words': 0, 'definitive_words': 0, 'uncertain_words': 0}\n",
    "        \n",
    "        input_text = str(input_text).lower()\n",
    "        output_text = str(output_text).lower()\n",
    "        \n",
    "        # Numbers that appear in output but not input\n",
    "        input_numbers = set(re.findall(r'\\b\\d+\\b', input_text))\n",
    "        output_numbers = set(re.findall(r'\\b\\d+\\b', output_text))\n",
    "        new_numbers = len(output_numbers - input_numbers)\n",
    "        \n",
    "        # Capitalized words (potential named entities)\n",
    "        input_caps = set(re.findall(r'\\b[A-Z][a-z]+\\b', str(input_text)))\n",
    "        output_caps = set(re.findall(r'\\b[A-Z][a-z]+\\b', str(output_text)))\n",
    "        new_caps_words = len(output_caps - input_caps)\n",
    "        \n",
    "        # Definitive and uncertain language\n",
    "        definitive_words = ['definitely', 'certainly', 'absolutely', 'clearly', 'obviously']\n",
    "        uncertain_words = ['might', 'could', 'possibly', 'perhaps', 'maybe', 'allegedly']\n",
    "        \n",
    "        definitive_count = sum(output_text.count(word) for word in definitive_words)\n",
    "        uncertain_count = sum(output_text.count(word) for word in uncertain_words)\n",
    "        \n",
    "        return {\n",
    "            'new_numbers': new_numbers,\n",
    "            'new_caps_words': new_caps_words,\n",
    "            'definitive_words': definitive_count,\n",
    "            'uncertain_words': uncertain_count\n",
    "        }\n",
    "    \n",
    "    def process_record(self, record):\n",
    "        \"\"\"Process a single record and extract all features.\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Extract texts\n",
    "        input_text = record.get('model_input', '')\n",
    "        output_text = record.get('model_output_text', '')\n",
    "        logits = record.get('model_output_logits', [])\n",
    "        \n",
    "        # Metadata\n",
    "        features['model_id'] = record.get('model_id', '')\n",
    "        features['lang'] = record.get('lang', 'unknown')\n",
    "        \n",
    "        # Basic features for input and output\n",
    "        input_features = self.extract_basic_features(input_text)\n",
    "        output_features = self.extract_basic_features(output_text)\n",
    "        \n",
    "        # Add prefixes\n",
    "        for key, value in input_features.items():\n",
    "            features[f'input_{key}'] = value\n",
    "        for key, value in output_features.items():\n",
    "            features[f'output_{key}'] = value\n",
    "        \n",
    "        # Similarity features\n",
    "        similarity_features = self.extract_similarity_features(input_text, output_text)\n",
    "        features.update(similarity_features)\n",
    "        \n",
    "        # Logits features\n",
    "        logits_features = self.extract_logits_features(logits)\n",
    "        features.update(logits_features)\n",
    "        \n",
    "        # Hallucination indicators\n",
    "        hallucination_features = self.extract_hallucination_indicators(input_text, output_text)\n",
    "        features.update(hallucination_features)\n",
    "        \n",
    "        # Labels (if available)\n",
    "        if 'hard_labels' in record:\n",
    "            features['label'] = record['hard_labels']\n",
    "        if 'soft_labels' in record:\n",
    "            features['soft_label'] = record['soft_labels']\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def process_dataset(self, dataset_data, dataset_name):\n",
    "        \"\"\"Process entire dataset.\"\"\"\n",
    "        logger.info(f\"Processing {dataset_name}\")\n",
    "        \n",
    "        all_records = []\n",
    "        for filename, file_data in dataset_data.items():\n",
    "            data = file_data['data']\n",
    "            if isinstance(data, list):\n",
    "                all_records.extend(data)\n",
    "            elif isinstance(data, dict):\n",
    "                for sub_key, sub_data in data.items():\n",
    "                    if isinstance(sub_data, list):\n",
    "                        all_records.extend(sub_data)\n",
    "        \n",
    "        processed_features = []\n",
    "        for i, record in enumerate(all_records):\n",
    "            try:\n",
    "                features = self.process_record(record)\n",
    "                features['dataset'] = dataset_name\n",
    "                processed_features.append(features)\n",
    "                \n",
    "                if (i + 1) % 500 == 0:\n",
    "                    logger.info(f\"Processed {i + 1}/{len(all_records)} records\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing record {i}: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(processed_features)\n",
    "\n",
    "class SimpleModelTrainer:\n",
    "    \"\"\"Simple model trainer for hallucination detection.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def prepare_features(self, df_list, target_col='label'):\n",
    "        \"\"\"Prepare features for modeling.\"\"\"\n",
    "        # Combine all dataframes\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        \n",
    "        # Separate features from metadata\n",
    "        feature_cols = [col for col in combined_df.columns \n",
    "                       if col not in ['model_id', 'lang', 'dataset', 'label', 'soft_label']]\n",
    "        \n",
    "        X = combined_df[feature_cols].fillna(0)\n",
    "        y = combined_df[target_col] if target_col in combined_df.columns else None\n",
    "        \n",
    "        return X, y, feature_cols, combined_df\n",
    "        \n",
    "    def train_models(self, X_train, y_train):\n",
    "        \"\"\"Train classification models.\"\"\"\n",
    "        models = {\n",
    "            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            'logistic_regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'svm': SVC(probability=True, random_state=42)\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        trained_models = {}\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            logger.info(f\"Training {name}\")\n",
    "            \n",
    "            # Cross-validation scores\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')\n",
    "            \n",
    "            # Train on full dataset\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            results[name] = {\n",
    "                'cv_f1_mean': np.mean(cv_scores),\n",
    "                'cv_f1_std': np.std(cv_scores)\n",
    "            }\n",
    "            trained_models[name] = model\n",
    "            \n",
    "            logger.info(f\"{name}: CV F1 = {results[name]['cv_f1_mean']:.3f} (+/- {results[name]['cv_f1_std']:.3f})\")\n",
    "        \n",
    "        self.models = trained_models\n",
    "        self.results = results\n",
    "        return trained_models, results\n",
    "    \n",
    "    def evaluate_on_test(self, X_test, y_test):\n",
    "        \"\"\"Evaluate trained models on test set.\"\"\"\n",
    "        test_results = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "            \n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "            \n",
    "            test_results[name] = {\n",
    "                'f1_score': f1,\n",
    "                'auc_score': auc,\n",
    "                'classification_report': classification_report(y_test, y_pred)\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"{name} Test Results: F1={f1:.3f}, AUC={auc:.3f if auc else 'N/A'}\")\n",
    "        \n",
    "        return test_results\n",
    "\n",
    "def run_simple_pipeline(loader_data):\n",
    "    \"\"\"Run the complete simple pipeline.\"\"\"\n",
    "    logger.info(\"Starting Simple Mu-SHROOM Pipeline\")\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = SimpleMuShroomProcessor()\n",
    "    \n",
    "    # Process all datasets\n",
    "    processed_datasets = {}\n",
    "    for dataset_name, dataset_data in loader_data.items():\n",
    "        df = processor.process_dataset(dataset_data, dataset_name)\n",
    "        processed_datasets[dataset_name] = df\n",
    "        logger.info(f\"Processed {dataset_name}: {len(df)} records, {len(df.columns)} features\")\n",
    "    \n",
    "    # Prepare for modeling\n",
    "    trainer = SimpleModelTrainer()\n",
    "    \n",
    "    # Check what data we have\n",
    "    train_df = processed_datasets.get('train_data')\n",
    "    test_labeled_df = processed_datasets.get('test_labeled') \n",
    "    test_unlabeled_df = processed_datasets.get('test_unlabeled')\n",
    "    \n",
    "    logger.info(\"Dataset summary:\")\n",
    "    if train_df is not None:\n",
    "        logger.info(f\"  Training data: {len(train_df)} records\")\n",
    "    if test_labeled_df is not None:\n",
    "        logger.info(f\"  Test labeled: {len(test_labeled_df)} records\")\n",
    "        logger.info(f\"  Labels available: {'label' in test_labeled_df.columns}\")\n",
    "    if test_unlabeled_df is not None:\n",
    "        logger.info(f\"  Test unlabeled: {len(test_unlabeled_df)} records\")\n",
    "    \n",
    "    # If we have labeled test data, use it for supervised learning\n",
    "    if test_labeled_df is not None and 'label' in test_labeled_df.columns:\n",
    "        X, y, feature_cols, combined_df = trainer.prepare_features([test_labeled_df])\n",
    "        \n",
    "        # Split for training and testing\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Train models\n",
    "        models, cv_results = trainer.train_models(X_train_scaled, y_train)\n",
    "        test_results = trainer.evaluate_on_test(X_test_scaled, y_test)\n",
    "        \n",
    "        return {\n",
    "            'processed_datasets': processed_datasets,\n",
    "            'models': models,\n",
    "            'cv_results': cv_results,\n",
    "            'test_results': test_results,\n",
    "            'feature_columns': feature_cols,\n",
    "            'scaler': scaler\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        logger.info(\"No labeled data available for supervised learning\")\n",
    "        return {\n",
    "            'processed_datasets': processed_datasets,\n",
    "            'feature_columns': None,\n",
    "            'message': 'Data processed successfully, but no labels available for model training'\n",
    "        }\n",
    "\n",
    "# Usage:\n",
    "# results = run_simple_pipeline(loaded_data)\n",
    "print(\"Simple Mu-SHROOM pipeline ready! Use: results = run_simple_pipeline(loaded_data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f85fd5",
   "metadata": {
    "papermill": {
     "duration": 0.002974,
     "end_time": "2025-09-28T12:44:23.152404",
     "exception": false,
     "start_time": "2025-09-28T12:44:23.149430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6773262,
     "sourceId": 10898707,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6773269,
     "sourceId": 10898715,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6773275,
     "sourceId": 10898723,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.532786,
   "end_time": "2025-09-28T12:44:24.077910",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-28T12:44:14.545124",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
